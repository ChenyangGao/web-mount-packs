#!/usr/bin/env python3
# encoding: utf-8

# NOTE: ä»¥ä¸‹è¿™äº›æ˜¯å¾…å®ç°çš„è®¾æƒ³ ğŸ‘‡
# TODO: ä½œä¸ºæ¨¡å—æä¾›ï¼Œå…è®¸å…¨é‡æ›´æ–°(updatedb)å’Œå¢é‡æ›´æ–°(updatedb_one)ï¼Œä½†åªå…è®¸åŒæ—¶æœ€å¤šä¸€ä¸ªå†™å…¥ä»»åŠ¡
# TODO: å¯ä»¥èµ·ä¸€ä¸ªæœåŠ¡ï¼Œå…¶å®ƒçš„ç¨‹åºï¼Œå¯ä»¥å‘é€è¯»å†™ä»»åŠ¡è¿‡æ¥ï¼Œæ•°æ®åº“å¯ä»¥ä»¥ fuse æˆ– webdav å±•ç¤º
# TODO: æ”¯æŒå¤šä¸ªä¸åŒç™»å½•è®¾å¤‡å¹¶å‘
# TODO: æ”¯æŒåŒä¸€ä¸ª cookies å¹¶å‘å› å­ï¼Œé»˜è®¤å€¼ 1
# TODO: ä½¿ç”¨åç¨‹è¿›è¡Œå¹¶å‘ï¼Œè€Œéå¤šçº¿ç¨‹
# TODO: å¦‚æœè¯·æ±‚è¶…æ—¶ï¼Œåˆ™éœ€è¦è¿›è¡Œé‡è¯•
# TODO: ä½¿ç”¨ urllib3 æ›¿ä»£ httpxï¼Œå¢åŠ ç¨³å®šæ€§
# TODO: å…è®¸ä½¿ç”¨æ‰¹é‡æ‹‰å–æ–¹æ³•ï¼Œè€Œé¿å…é€’å½’
# TODO: sqlite çš„æ•°æ®åº“äº‹åŠ¡å’Œå†™å…¥ä¼šè‡ªåŠ¨åŠ é”ï¼Œå¦‚æœæœ‰å¤šä¸ªç¨‹åºåœ¨å¹¶å‘ï¼Œåˆ™å¯ä»¥ç­‰å¾…é”ï¼Œéœ€è¦ä¸€ä¸ªè¶…æ—¶æ—¶é—´å’Œé‡è¯•æ¬¡æ•°

__author__ = "ChenyangGao <https://chenyanggao.github.io>"
__version__ = (0, 0, 4)
__all__ = ["updatedb", "updatedb_one"]
__doc__ = "éå† 115 ç½‘ç›˜çš„ç›®å½•ä¿¡æ¯å¯¼å‡ºåˆ°æ•°æ®åº“"
__requirements__ = ["orjson", "python-115", "posixpatht"]

if __name__ == "__main__":
    from argparse import ArgumentParser, RawTextHelpFormatter

    parser = ArgumentParser(
        formatter_class=RawTextHelpFormatter, 
        description=__doc__, 
    )
    parser.add_argument("top_dirs", metavar="dir", nargs="*", help="""\
115 ç›®å½•ï¼Œå¯ä»¥ä¼ å…¥å¤šä¸ªï¼Œå¦‚æœä¸ä¼ é»˜è®¤ä¸º 0
å…è®¸ 3 ç§ç±»å‹çš„ç›®å½•
    1. æ•´æ•°ï¼Œè§†ä¸ºç›®å½•çš„ id
    2. å½¢å¦‚ "/åå­—/åå­—/..." çš„è·¯å¾„ï¼Œæœ€å‰é¢çš„ "/" å¯ä»¥çœç•¥ï¼Œæœ¬ç¨‹åºä¼šå°è¯•è·å–å¯¹åº”çš„ id
    3. å½¢å¦‚ "æ ¹ç›®å½• > åå­— > åå­— > ..." çš„è·¯å¾„ï¼Œæ¥è‡ªç‚¹å‡»æ–‡ä»¶çš„ã€æ˜¾ç¤ºå±æ€§ã€‘ï¼Œåœ¨ã€ä½ç½®ã€‘è¿™éƒ¨åˆ†çœ‹åˆ°çš„è·¯å¾„ï¼Œæœ¬ç¨‹åºä¼šå°è¯•è·å–å¯¹åº”çš„ id
""")
    parser.add_argument("-c", "--cookies", help="115 ç™»å½• cookiesï¼Œä¼˜å…ˆçº§é«˜äº -cp/--cookies-path")
    parser.add_argument("-cp", "--cookies-path", help="""\
å­˜å‚¨ 115 ç™»å½• cookies çš„æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„ï¼Œå¦‚æœç¼ºå¤±ï¼Œåˆ™ä» 115-cookies.txt æ–‡ä»¶ä¸­è·å–ï¼Œæ­¤æ–‡ä»¶å¯åœ¨å¦‚ä¸‹ç›®å½•ä¹‹ä¸€: 
    1. å½“å‰å·¥ä½œç›®å½•
    2. ç”¨æˆ·æ ¹ç›®å½•
    3. æ­¤è„šæœ¬æ‰€åœ¨ç›®å½•
å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ '2. ç”¨æˆ·æ ¹ç›®å½•ï¼Œæ­¤æ—¶åˆ™éœ€è¦æ‰«ç ç™»å½•'""")
    parser.add_argument("-f", "--dbfile", default="", help="sqlite æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºåœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ f'115-{user_id}.db'")
    parser.add_argument("-cl", "--clean", action="store_true", help="ä»»åŠ¡å®Œæˆåæ¸…ç†æ•°æ®åº“ï¼Œä»¥èŠ‚çº¦ç©ºé—´")
    parser.add_argument("-nr", "--not-recursive", action="store_true", help="ä¸éå†ç›®å½•æ ‘ï¼šåªæ‹‰å–é¡¶å±‚ç›®å½•ï¼Œä¸é€’å½’å­ç›®å½•")
    parser.add_argument("-r", "--resume", action="store_true", help="""ä¸­æ–­é‡è¯•ï¼Œåˆ¤æ–­ä¾æ®ï¼ˆæ»¡è¶³å¦‚ä¸‹æ¡ä»¶ä¹‹ä¸€ï¼‰ï¼š
    1. é¡¶å±‚ç›®å½•æœªè¢«é‡‡é›†ï¼šå‘½ä»¤è¡Œæ‰€æŒ‡å®šçš„æŸä¸ª dir_id çš„æ–‡ä»¶åˆ—è¡¨æœªè¢«é‡‡é›†
    2. ç›®å½•æœªè¢«é‡‡é›†ï¼šæŸä¸ªç›®å½•å†…çš„æ–‡ä»¶åˆ—è¡¨ä¸ºç©ºï¼ˆå¯èƒ½ä¸ºç©ºï¼Œä¹Ÿå¯èƒ½æœªè¢«é‡‡é›†ï¼‰
    3. ç›®å½•æ›´æ–°è‡³æ­¤ï¼šæŸä¸ªç›®å½•çš„æ–‡ä»¶ä¿¡æ¯çš„æ›´æ–°æ—¶é—´å¤§äºå®ƒé‡Œé¢çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ä¸­æ›´æ–°æ—¶é—´æœ€å¤§çš„é‚£ä¸€æ¡
""")
    parser.add_argument("-v", "--version", action="store_true", help="è¾“å‡ºç‰ˆæœ¬å·")

    args = parser.parse_args()
    if args.version:
        print(".".join(map(str, __version__)))
        raise SystemExit(0)

import logging

from collections import deque, ChainMap
from collections.abc import Collection, Iterator, Iterable
from errno import EBUSY, ENOENT, ENOTDIR
from os.path import splitext
from sqlite3 import (
    connect, register_adapter, register_converter, Connection, Cursor, 
    Row, PARSE_COLNAMES, PARSE_DECLTYPES
)
from typing import cast

try:
    from orjson import dumps, loads
    from p115 import check_response, P115Client
    from posixpatht import escape, joins, normpath
except ImportError:
    from sys import executable
    from subprocess import run
    run([executable, "-m", "pip", "install", "-U", *__requirements__], check=True)
    from orjson import dumps, loads
    from p115 import check_response, P115Client
    from posixpatht import escape, joins, normpath


register_adapter(list, dumps)
register_adapter(dict, dumps)
register_converter("JSON", loads)

logger = logging.Logger("115-updatedb", level=logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter(
    "[\x1b[1m%(asctime)s\x1b[0m] (\x1b[1;36m%(levelname)s\x1b[0m) "
    "\x1b[0m\x1b[1;35m%(name)s\x1b[0m \x1b[5;31mâœ\x1b[0m %(message)s"
))
logger.addHandler(handler)


class OSBusyError(OSError):

    def __init__(self, *args):
        super().__init__(EBUSY, *args)


def cut_iter(
    start: int, 
    stop: None | int = None, 
    step: int = 1, 
) -> Iterator[tuple[int, int]]:
    if stop is None:
        start, stop = 0, start
    for mid in range(start + step, stop, step):
        yield start, step
        start = mid
    if start < stop:
        yield start, stop - start


def normalize_path(path: str, /):
    if path in ("0", ".", "..", "/"):
        return 0
    if path.isdecimal():
        return int(path)
    if path.startswith("æ ¹ç›®å½• > "):
        patht = path.split(" > ")
        patht[0] = ""
        return joins(patht)
    if not path.startswith("/"):
        path = "/" + path
    return normpath(path)


def do_commit(
    con: Connection | Cursor, 
):
    conn = cast(Connection, getattr(con, "connection", con))
    conn.commit()


def execute_commit(
    con: Connection | Cursor, 
    /, 
    sql: str, 
    params = None, 
    executemany: bool = False, 
) -> Cursor:
    conn = cast(Connection, getattr(con, "connection", con))
    try:
        if executemany:
            cur = con.executemany(sql, params)
        elif params is None:
            cur = con.execute(sql)
        else:
            cur = con.execute(sql, params)
        conn.commit()
        return cur
    except BaseException:
        conn.rollback()
        raise


def json_array_head_replace(value, repl, stop=None):
    value = loads(value)
    repl  = loads(repl)
    if stop is None:
        stop = len(repl)
    value[:stop] = repl
    return dumps(value)


def initdb(con: Connection | Cursor, /) -> Cursor:
    conn = cast(Connection, getattr(con, "connection", con))
    conn.row_factory = Row
    conn.create_function("escape_name", 1, escape)
    conn.create_function("json_array_head_replace", 3, json_array_head_replace)
    dbpath = con.execute("SELECT file FROM pragma_database_list() WHERE name='main';").fetchone()[0]
    file_dbpath = "%s-file%s" % splitext(dbpath)
    con.execute("ATTACH DATABASE ? AS file;", (file_dbpath,))
    try:
        con2 = connect(file_dbpath)
        con2.execute("PRAGMA journal_mode = WAL;")
    finally:
        con2.close()
    return con.executescript("""\
PRAGMA journal_mode = WAL;

CREATE TABLE IF NOT EXISTS data (
    id INTEGER NOT NULL PRIMARY KEY,
    parent_id INTEGER NOT NULL,
    pickcode TEXT NOT NULL DEFAULT '',
    name TEXT NOT NULL,
    size INTEGER NOT NULL DEFAULT 0,
    sha1 TEXT NOT NULL DEFAULT '',
    is_dir INTEGER NOT NULL CHECK(is_dir IN (0, 1)),
    is_image INTEGER NOT NULL CHECK(is_image IN (0, 1)) DEFAULT 0,
    ctime INTEGER NOT NULL DEFAULT 0,
    mtime INTEGER NOT NULL DEFAULT 0,
    path TEXT NOT NULL DEFAULT '',
    ancestors JSON NOT NULL DEFAULT '',
    updated_at DATETIME DEFAULT (strftime('%Y-%m-%dT%H:%M:%S.%f+08:00', 'now', '+8 hours'))
);

CREATE TABLE IF NOT EXISTS file.data (
    id INTEGER NOT NULL PRIMARY KEY,
    data BLOB,
    temp_path TEXT
);

CREATE TRIGGER IF NOT EXISTS trg_data_updated_at
AFTER UPDATE ON data 
FOR EACH ROW
BEGIN
    UPDATE data SET updated_at = strftime('%Y-%m-%dT%H:%M:%S.%f+08:00', 'now', '+8 hours') WHERE id = NEW.id;
END;

CREATE INDEX IF NOT EXISTS idx_data_parent_id ON data(parent_id);
CREATE INDEX IF NOT EXISTS idx_data_path ON data(path);
""")


def select_ids_to_update(
    con: Connection | Cursor, 
    top_dirs: int | Iterable[int] = 0, 
    /, 
) -> Cursor:
    if isinstance(top_dirs, int):
        ids = "(%d)" % top_dirs
    else:
        ids = ",".join(map("(%d)".__mod__, top_dirs))
        if not ids:
            raise ValueError("no top_dirs specified")
    sql = f"""\
WITH top_dir_ids(id) AS (
    VALUES {ids}
), ids_to_update AS (
    SELECT
        d1.id, 
        d1.updated_at, 
        MAX(d2.updated_at) AS max_sub_updated_at
    FROM
        data d1 LEFT JOIN data d2 ON (d1.id=d2.parent_id)
    WHERE
        d1.is_dir
        AND d1.mtime
        AND (d2.mtime OR d2.id IS NULL)
    GROUP BY
        d1.id
    HAVING
        max_sub_updated_at IS NULL OR d1.updated_at > max_sub_updated_at
)
SELECT top.id FROM top_dir_ids AS top WHERE NOT EXISTS(SELECT 1 FROM data WHERE parent_id = top.id AND mtime)
UNION ALL
SELECT id FROM ids_to_update;
"""
    return con.execute(sql)


def select_subdir_ids(
    con: Connection | Cursor, 
    parent_id: int = 0, 
    /, 
) -> Cursor:
    sql = "SELECT id FROM data WHERE parent_id=? AND is_dir=1;"
    return con.execute(sql, (parent_id,))


def select_mtime_groups(
    con: Connection | Cursor, 
    parent_id: int = 0, 
    /, 
) -> Cursor:
    sql = """\
SELECT mtime, JSON_GROUP_ARRAY(id) AS "ids [JSON]"
FROM data
WHERE parent_id=? AND mtime != 0
GROUP BY mtime
ORDER BY mtime DESC;
"""
    return con.execute(sql, (parent_id,))


def update_dir_ancestors(
    con: Connection | Cursor, 
    ancestors: list[dict], 
    to_replace: list[dict] = [], 
    /, 
    commit: bool = True, 
) -> Cursor:
    if isinstance(con, Cursor):
        cur = con
        con = cur.connection
    else:
        cur = con.cursor()
    items1: dict[int, dict] = {}
    items2: dict[int, dict] = {}
    items = ChainMap(items1, items2)
    path = ""
    for i, a in enumerate(ancestors[1:], 2):
        path += "/" + escape(a["name"])
        items1[a["id"]] = {
            "id": a["id"], 
            "parent_id": a["parent_id"], 
            "name": a["name"], 
            "is_dir": 1, 
            "path": path, 
            "ancestors": ancestors[:i], 
        }
    for a in to_replace:
        if a["is_dir"]:
            items2[a["id"]] = {
                "id": a["id"], 
                "parent_id": a["parent_id"], 
                "name": a["name"], 
                "is_dir": 1, 
                "path": path + "/" + escape(a["name"]), 
                "ancestors": [*ancestors, {"id": a["id"], "parent_id": a["parent_id"], "name": a["name"]}], 
            }
    if not items:
        return cur
    sql = f"""\
SELECT id, parent_id, name, path, JSON_ARRAY_LENGTH(ancestors) AS ancestors_length
FROM data
WHERE id IN ({','.join(map(str, items))})
ORDER BY LENGTH(path) DESC;
"""
    changed = []
    for row in cur.execute(sql):
        cid = row["id"]
        new = items[cid]
        if row["name"] != new["name"] or row["parent_id"] != new["parent_id"]:
            changed.append({
                "path_old": row["path"], 
                "path_old_stop": len(row["path"]) + 1, 
                "path": new["path"], 
                "ancestors_old_stop": row["ancestors_length"], 
                "ancestors": new["ancestors"], 
            })
    try:
        if changed:
            sql = """\
UPDATE data
SET
    path = :path || SUBSTR(path, :path_old_stop), 
    ancestors = json_array_head_replace(ancestors, :ancestors, :ancestors_old_stop)
WHERE
    path LIKE :path_old || '/%'
"""
            cur.executemany(sql, changed)
        sql = """\
INSERT INTO
    data(id, parent_id, name, is_dir, path, ancestors)
VALUES
    (:id, :parent_id, :name, :is_dir, :path, :ancestors)
ON CONFLICT(id) DO UPDATE SET
    parent_id = excluded.parent_id,
    name      = excluded.name,
    path      = excluded.path,
    ancestors = excluded.ancestors
WHERE
    path != excluded.path;
"""
        cur.executemany(sql, items1.values())
        if commit:
            con.commit()
        return cur
    except BaseException:
        if commit:
            con.rollback()
        raise


def insert_items(
    con: Connection | Cursor, 
    items: dict | Iterable[dict], 
    /, 
    commit: bool = True, 
) -> Cursor:
    sql = """\
INSERT INTO
    data(id, parent_id, pickcode, name, size, sha1, is_dir, is_image, ctime, mtime)
VALUES
    (:id, :parent_id, :pickcode, :name, :size, :sha1, :is_dir, :is_image, :ctime, :mtime)
ON CONFLICT(id) DO UPDATE SET
    parent_id = excluded.parent_id,
    pickcode  = excluded.pickcode,
    name      = excluded.name,
    ctime     = excluded.ctime,
    mtime     = excluded.mtime
WHERE
    mtime != excluded.mtime
"""
    if isinstance(items, dict):
        items = items,
    if commit:
        return execute_commit(con, sql, items, executemany=True)
    else:
        return con.executemany(sql, items)


def delete_items(
    con: Connection | Cursor, 
    ids: int | Iterable[int], 
    /, 
    commit: bool = True, 
) -> Cursor:
    if isinstance(ids, int):
        cond = f"id = {ids:d}"
    else:
        cond = "id IN (%s)" % (",".join(map(str, ids)) or "NULL")
    sql = f"DELETE FROM data WHERE {cond}"
    if commit:
        return execute_commit(con, sql)
    else:
        return con.execute(sql)


def update_files_time(
    con: Connection | Cursor, 
    parent_id: int = 0, 
    /, 
    commit: bool = True, 
) -> Cursor:
    sql = """\
UPDATE data
SET updated_at = strftime('%Y-%m-%dT%H:%M:%S.%f+08:00', 'now', '+8 hours')
WHERE parent_id = ?;
"""
    if commit:
        return execute_commit(con, sql, (parent_id,))
    else:
        return con.execute(sql, (parent_id,))


def update_path(
    con: Connection | Cursor, 
    parent_id: int = 0, 
    /, 
    ancestors: list[dict] = [], 
    commit: bool = True, 
) -> Cursor:
    sql = """\
UPDATE data
SET
    ancestors = JSON_INSERT(?, '$[#]', JSON_OBJECT('id', id, 'parent_id', parent_id, 'name', name)), 
    path = ? || escape_name(name)
WHERE parent_id=?;
"""
    dirname = "/".join(a["name"] for a in ancestors) + "/"
    if commit:
        return execute_commit(con, sql, (ancestors, dirname, parent_id))
    else:
        return con.execute(sql, (ancestors, dirname, parent_id))


def find_dangling_ids(
    con: Connection | Cursor, 
    /, 
) -> set[int]:
    d = dict(con.execute("SELECT id, parent_id FROM data;"))
    temp: list[int] = []
    ok_ids: set[int] = set()
    na_ids: set[int] = set()
    push = temp.append
    clear = temp.clear
    update_ok = ok_ids.update
    update_na = na_ids.update
    for k, v in d.items():
        try:
            push(k)
            while k := d[k]:
                if k in ok_ids:
                    update_ok(temp)
                    break
                elif k in na_ids:
                    update_na(temp)
                    break
                push(k)
            else:
                update_ok(temp)
        except KeyError:
            update_na(temp)
        finally:
            clear()
    return na_ids


def cleandb(
    con: Connection | Cursor, 
    /, 
    commit: bool = True, 
) -> Cursor:
    return delete_items(con, find_dangling_ids(con), commit=commit)


def normalize_attr(info: dict, /) -> dict:
    is_dir = "fid" not in info
    if is_dir:
        attr: dict = {"id": int(info["cid"]), "parent_id": int(info["pid"])}
    else:
        attr = {"id": int(info["fid"]), "parent_id": int(info["cid"])}
    attr["pickcode"] = info["pc"]
    attr["name"] = info["n"]
    attr["size"] = info.get("s") or 0
    attr["sha1"] = info.get("sha") or ""
    attr["is_dir"] = is_dir
    attr["is_image"] = not is_dir and bool(info.get("u"))
    attr["ctime"] = int(info.get("tp", 0))
    attr["mtime"] = int(info.get("te", 0))
    return attr


def iterdir(
    client: P115Client, 
    id: int = 0, 
    /, 
    page_size: int = 1024, 
) -> tuple[int, list[dict], Iterator[dict]]:
    if page_size <= 0:
        page_size = 1024
    payload = {
        "asc": 0, "cid": id, "custom_order": 1, "fc_mix": 1, "limit": min(16, page_size), 
        "show_dir": 1, "o": "user_utime", "offset": 0, 
    }
    fs_files = client.fs_files
    count = -1
    ancestors = [{"id": 0, "parent_id": 0, "name": ""}]

    def get_files():
        nonlocal count
        resp = check_response(fs_files(payload))
        if int(resp["path"][-1]["cid"]) != id:
            if count < 0:
                raise NotADirectoryError(ENOTDIR, f"not a dir or deleted: {id}")
            else:
                raise FileNotFoundError(ENOENT, f"no such dir: {id}")
        ancestors[1:] = (
            {"id": int(info["cid"]), "parent_id": int(info["pid"]), "name": info["name"]} 
            for info in resp["path"][1:]
        )
        if count < 0:
            count = resp["count"]
        elif count != resp["count"]:
            raise OSBusyError(f"detected count changes during iteration: {id}")
        return resp

    resp = get_files()

    def iter():
        nonlocal resp
        offset = 0
        payload["limit"] = page_size
        while True:
            yield from map(normalize_attr, resp["data"])
            offset += len(resp["data"])
            if offset >= count:
                break
            payload["offset"] = offset
            resp = get_files()

    return count, ancestors, iter()


def diff_dir(
    con: Connection | Cursor, 
    client: P115Client, 
    id: int = 0, 
    /, 
):
    n = 0
    saved: dict[int, set[int]] = {}
    for mtime, ls in select_mtime_groups(con, id):
        saved[mtime] = set(ls)
        n += len(ls)

    replace_list: list[dict] = []
    delete_list: list[int] = []
    count, ancestors, data_it = iterdir(client, id)
    if not n:
        replace_list.extend(data_it)
        return ancestors, delete_list, replace_list

    seen: set[int] = set()
    seen_add = seen.add
    it = iter(saved.items())
    his_mtime, his_ids = next(it)
    for attr in data_it:
        cur_id = attr["id"]
        if cur_id in seen:
            raise OSBusyError(f"duplicate id found: {cur_id}")
        seen_add(cur_id)
        cur_mtime = attr["mtime"]
        while his_mtime > cur_mtime:
            delete_list.extend(his_ids - seen)
            n -= len(his_ids)
            if not n:
                replace_list.append(attr)
                replace_list.extend(data_it)
                return ancestors, delete_list, replace_list
            his_mtime, his_ids = next(it)
        if his_mtime == cur_mtime:
            if cur_id in his_ids:
                n -= 1
                if count - len(seen) == n:
                    return ancestors, delete_list, replace_list
                his_ids.remove(cur_id)
        else:
            replace_list.append(attr)
    for _, his_ids in it:
        delete_list.extend(his_ids - seen)
    return ancestors, delete_list, replace_list


def updatedb_one(
    client: str | P115Client, 
    dbfile: None | str | Connection | Cursor = None, 
    id: int = 0, 
):
    if isinstance(client, str):
        client = P115Client(client, check_for_relogin=True)
    if not dbfile:
        dbfile = f"115-{client.user_id}.db"
    if isinstance(dbfile, (Connection, Cursor)):
        con = dbfile
        try:
            ancestors, to_delete, to_replace = diff_dir(con, client, id)
            logger.info("[\x1b[1;32mGOOD\x1b[0m] %s", id)
        except BaseException as e:
            logger.exception("[\x1b[1;31mFAIL\x1b[0m] %s", id)
            if isinstance(e, (FileNotFoundError, NotADirectoryError)):
                delete_items(con, id)
            raise
        else:
            update_dir_ancestors(con, ancestors, to_replace, commit=False)
            if to_delete:
                delete_items(con, to_delete, commit=False)
            if to_replace:
                insert_items(con, to_replace, commit=False)
                update_path(con, id, ancestors=ancestors, commit=False)
            update_files_time(con, id, commit=False)
            do_commit(con)
    else:
        with connect(
            dbfile, 
            detect_types=PARSE_DECLTYPES|PARSE_COLNAMES, 
            uri=dbfile.startswith("file:"), 
        ) as con:
            initdb(con)
            updatedb_one(client, con, id)


def updatedb(
    client: str | P115Client, 
    dbfile: None | str | Connection | Cursor = None, 
    top_dirs: int | str | Iterable[int | str] = 0, 
    recursive: bool = True, 
    resume: bool = False, 
    clean: bool = False, 
):
    if isinstance(client, str):
        client = P115Client(client, check_for_relogin=True)
    if not dbfile:
        dbfile = f"115-{client.user_id}.db"
    if isinstance(dbfile, (Connection, Cursor)):
        con = dbfile
        seen: set[int] = set()
        seen_add = seen.add
        dq: deque[int] = deque()
        push, pop = dq.append, dq.popleft
        if isinstance(top_dirs, int):
            top_ids: Collection[int] = (top_dirs,)
        elif isinstance(top_dirs, str):
            try:
                top_ids = (client.fs.get_id(normalize_path(top_dirs)),)
            except:
                logger.exception("[\x1b[1;31mFAIL\x1b[0m] %s", top_dirs)
                return
        else:
            top_ids = set()
            for top_dir in top_dirs:
                if isinstance(top_dir, int):
                    top_ids.add(top_dir)
                else:
                    try:
                        top_ids.add(client.fs.get_id(normalize_path(top_dir)))
                    except:
                        logger.exception("[\x1b[1;31mFAIL\x1b[0m] %s", top_dir)
                        continue
            if not top_ids:
                return
        if resume:
            dq.extend(r[0] for r in select_ids_to_update(con, top_ids))
        else:
            dq.extend(top_ids)
        while dq:
            id = pop()
            if id in seen:
                logger.warning("[\x1b[1;33mSKIP\x1b[0m]", id)
                continue
            try:
                updatedb_one(client, con, id)
            except (FileNotFoundError, NotADirectoryError):
                pass
            except OSBusyError:
                logger.warning("[\x1b[1;34mREDO\x1b[0m] %s", id)
                push(id)
            else:
                seen_add(id)
                if recursive:
                    dq.extend(r[0] for r in select_subdir_ids(con, id))
        if clean and top_ids:
            cleandb(con)
    else:
        with connect(
            dbfile, 
            detect_types=PARSE_DECLTYPES|PARSE_COLNAMES, 
            uri=dbfile.startswith("file:"), 
        ) as con:
            initdb(con)
            updatedb(
                client, 
                con, 
                top_dirs=top_dirs, 
                recursive=recursive, 
                resume=resume, 
                clean=clean, 
            )
            if clean:
                con.execute("PRAGMA wal_checkpoint;")
                con.execute("VACUUM;")


if __name__ == "__main__":
    if args.cookies:
        cookies = args.cookies
    else:
        from pathlib import Path

        if args.cookies_path:
            cookies = Path(args.cookies_path).absolute()
        else:
            for path in (
                Path("./115-cookies.txt").absolute(), 
                Path("~/115-cookies.txt").expanduser(), 
                Path(__file__).parent / "115-cookies.txt", 
            ):
                if path.is_file():
                    cookies = path
            else:
                cookies = Path("~/115-cookies.txt").expanduser()
    client = P115Client(cookies, check_for_relogin=True)
    updatedb(
        client, 
        dbfile=args.dbfile, 
        recursive=not args.not_recursive, 
        resume=args.resume, 
        top_dirs=args.top_dirs or 0, 
        clean=args.clean, 
    )


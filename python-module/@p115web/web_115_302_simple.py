#!/usr/bin/env python3
# encoding: utf-8

__author__ = "ChenyangGao <https://chenyanggao.github.io>"
__version__ = (0, 0, 9)
__requirements__ = ["blacksheep", "cachetools", "orjson", "p115cipher", "uvicorn"]

from os.path import dirname, expanduser, join as joinpath, realpath

__doc__ = """\
        \x1b[5mğŸš€\x1b[0m 115 ç›´é“¾æœåŠ¡ç®€å•ä¸”æé€Ÿç‰ˆ \x1b[5mğŸ³\x1b[0m

é“¾æ¥æ ¼å¼ï¼ˆæ¯ä¸ªå‚æ•°éƒ½æ˜¯\x1b[1;31må¯é€‰çš„\x1b[0mï¼‰ï¼š\x1b[4m\x1b[34mhttp://localhost{\x1b[1;32mpath2\x1b[0m\x1b[4m\x1b[34m}?pickcode={\x1b[1;32mpickcode\x1b[0m\x1b[4m\x1b[34m}&id={\x1b[1;32mid\x1b[0m\x1b[4m\x1b[34m}&sha1={\x1b[1;32msha1\x1b[0m\x1b[4m\x1b[34m}&path={\x1b[1;32mpath\x1b[0m\x1b[4m\x1b[34m}&image={\x1b[1;32mimage\x1b[0m\x1b[4m\x1b[34m}&disable_pc={\x1b[1;32mdisable_pc\x1b[0m\x1b[4m\x1b[34m}\x1b[0m

- \x1b[1;32mpickcode\x1b[0m: æ–‡ä»¶çš„ \x1b[1;32mpickcode\x1b[0mï¼Œä¼˜å…ˆçº§é«˜äº \x1b[1;32mid\x1b[0m
- \x1b[1;32mid\x1b[0m: æ–‡ä»¶çš„ \x1b[1;32mid\x1b[0mï¼Œä¼˜å…ˆçº§é«˜äº \x1b[1;32msha1\x1b[0m
- \x1b[1;32msha1\x1b[0m: æ–‡ä»¶çš„ \x1b[1;32msha1\x1b[0mï¼Œä¼˜å…ˆçº§é«˜äº \x1b[1;32mpath\x1b[0m
- \x1b[1;32mpath\x1b[0m: æ–‡ä»¶çš„è·¯å¾„ï¼Œä¼˜å…ˆçº§é«˜äº \x1b[1;32mpath2\x1b[0m
- \x1b[1;32mimage\x1b[0m: æ¥å— \x1b[1;36m1\x1b[0m | \x1b[1;36mtrue\x1b[0m æˆ– \x1b[1;36m0\x1b[0m | \x1b[1;36mfalse\x1b[0mï¼Œå¦‚æœä¸º \x1b[1;36m1\x1b[0m | \x1b[1;36mtrue\x1b[0m ä¸”æä¾› \x1b[1;32mpickcode\x1b[0m ä¸”è®¾ç½®äº†ç¯å¢ƒå˜é‡ \x1b[1;32mcdn_image\x1b[0mï¼Œåˆ™è§†ä¸ºè¯·æ±‚å›¾ç‰‡
- \x1b[1;32mdisable_pc\x1b[0m: æ¥å— \x1b[1;36m1\x1b[0m | \x1b[1;36mtrue\x1b[0m æˆ– \x1b[1;36m0\x1b[0m | \x1b[1;36mfalse\x1b[0mï¼Œå¦‚æœä¸º \x1b[1;36m1\x1b[0m | \x1b[1;36mtrue\x1b[0mï¼Œåˆ™æ­¤æ¬¡è¯·æ±‚è§† \x1b[1;32mpath_persistence_commitment\x1b[0m ä¸º \x1b[1;36mFalse\x1b[0m

        \x1b[5mğŸŒ\x1b[0m ç¯å¢ƒå˜é‡ \x1b[5mğŸ›¸\x1b[0m

- \x1b[1;32mcookies\x1b[0m: 115 ç™»å½• cookiesï¼Œä¼˜å…ˆçº§é«˜äº \x1b[1;32mcookies_path\x1b[0m
- \x1b[1;32mcookies_path\x1b[0m: å­˜å‚¨ 115 ç™»å½• cookies çš„æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„ï¼Œå¦‚æœç¼ºå¤±ï¼Œåˆ™ä» \x1b[4m\x1b[34m115-cookies.txt\x1b[0m æ–‡ä»¶ä¸­è·å–ï¼Œæ­¤æ–‡ä»¶å¯ä»¥åœ¨å¦‚ä¸‹è·¯å¾„ä¹‹ä¸€ï¼ˆæŒ‰å…ˆåé¡ºåºï¼‰
    1. å½“å‰å·¥ä½œç›®å½•: \x1b[4m\x1b[34m%(file_in_cwd)s\x1b[0m
    2. ç”¨æˆ·æ ¹ç›®å½•: \x1b[4m\x1b[34m%(file_in_home)s\x1b[0m
    3. æ­¤è„šæœ¬æ‰€åœ¨ç›®å½•: \x1b[4m\x1b[34m%(file_in_dir)s\x1b[0m
- \x1b[1;32mpath_persistence_commitment\x1b[0m: ï¼ˆ\x1b[1;31mä¼ å…¥ä»»ä½•å€¼éƒ½è§†ä¸ºè®¾ç½®ï¼ŒåŒ…æ‹¬ç©ºå­—ç¬¦ä¸²\x1b[0mï¼‰è·¯å¾„æŒä¹…æ€§æ‰¿è¯ºï¼Œåªè¦ä½ èƒ½ä¿è¯æ–‡ä»¶ä¸ä¼šè¢«ç§»åŠ¨ï¼ˆ\x1b[1;31må¯æ–°å¢åˆ é™¤ï¼Œä½†å¯¹åº”çš„è·¯å¾„ä¸å¯è¢«å…¶ä»–æ–‡ä»¶å¤ç”¨\x1b[0mï¼‰ï¼Œæ‰“å¼€æ­¤é€‰é¡¹ï¼Œç”¨è·¯å¾„è¯·æ±‚ç›´é“¾æ—¶ï¼Œå¯èŠ‚çº¦ä¸€åŠæ—¶é—´
- \x1b[1;32mcdn_image\x1b[0m: ï¼ˆ\x1b[1;31mä¼ å…¥ä»»ä½•å€¼éƒ½è§†ä¸ºè®¾ç½®ï¼ŒåŒ…æ‹¬ç©ºå­—ç¬¦ä¸²\x1b[0mï¼‰å›¾ç‰‡èµ° cdnï¼Œè®¾ç½®æ­¤å‚æ•°ä¼šåˆ›å»ºä¸€ä¸ªå›¾ç‰‡ç›´é“¾çš„ç¼“å­˜
- \x1b[1;32mcdn_image_warmup_ids\x1b[0m: ä¸ºå›¾ç‰‡çš„ cdn ç¼“å­˜è¿›è¡Œé¢„çƒ­ï¼Œæ¥å—æ–‡ä»¶å¤¹ idï¼Œå¦‚æœæœ‰å¤šä¸ªç”¨é€—å·(\x1b[1;36m,\x1b[0m)éš”å¼€
- \x1b[1;32mcdn_image_warmup_no_path_cache\x1b[0m: ï¼ˆ\x1b[1;31mä¼ å…¥ä»»ä½•å€¼éƒ½è§†ä¸ºè®¾ç½®ï¼ŒåŒ…æ‹¬ç©ºå­—ç¬¦ä¸²\x1b[0mï¼‰ä¸ºå›¾ç‰‡çš„ cdn ç¼“å­˜è¿›è¡Œé¢„çƒ­æ—¶ï¼Œä¸å»ºç«‹è·¯å¾„åˆ° id çš„æ˜ å°„ï¼Œä»¥åŠ å¿«é¢„çƒ­é€Ÿåº¦ï¼Œä½†ä½¿ç”¨è·¯å¾„è·å–å›¾ç‰‡æ—¶é€Ÿåº¦æ…¢å¾ˆå¤š
- \x1b[1;32murl_ttl\x1b[0m: ç›´é“¾å­˜æ´»æ—¶é—´ï¼ˆ\x1b[1;31må•ä½ï¼šç§’\x1b[0mï¼‰ï¼Œé»˜è®¤å€¼ \x1b[1;36m1\x1b[0mã€‚ç‰¹åˆ«çš„ï¼Œè‹¥ \x1b[1;36m= 0\x1b[0mï¼Œåˆ™ä¸ç¼“å­˜ï¼›è‹¥ \x1b[1;36m< 0\x1b[0mï¼Œåˆ™ä¸é™æ—¶
- \x1b[1;32murl_reuse_factor\x1b[0m: ç›´é“¾æœ€å¤§å¤ç”¨æ¬¡æ•°ï¼Œé»˜è®¤å€¼ \x1b[1;36m-1\x1b[0mã€‚ç‰¹åˆ«çš„ï¼Œè‹¥ \x1b[1;36m= 0\x1b[0m æˆ– \x1b[1;36m= 1\x1b[0mï¼Œåˆ™ä¸ç¼“å­˜ï¼›è‹¥ \x1b[1;36m< 0\x1b[0mï¼Œåˆ™ä¸é™æ¬¡æ•°
- \x1b[1;32murl_range_request_cooldown\x1b[0m: range è¯·æ±‚å†·å´æ—¶é—´ï¼Œé»˜è®¤å€¼ \x1b[1;36m0\x1b[0mï¼ŒæŸä¸ª ip å¯¹æŸä¸ªèµ„æºæ‰§è¡Œä¸€æ¬¡ range è¯·æ±‚åå¿…é¡»è¿‡ä¸€å®šçš„å†·å´æ—¶é—´åæ‰èƒ½å¯¹ç›¸åŒèŒƒå›´å†æ¬¡è¯·æ±‚ã€‚ç‰¹åˆ«çš„ï¼Œè‹¥ \x1b[1;36m<= 0\x1b[0mï¼Œåˆ™ä¸éœ€è¦å†·å´

        \x1b[5mğŸ”¨\x1b[0m å¦‚ä½•è¿è¡Œ \x1b[5mğŸª›\x1b[0m

åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ª \x1b[4m\x1b[34m115-cookies.txt\x1b[0mï¼Œå¹¶æŠŠ 115 çš„ cookies ä¿å­˜å…¶ä¸­ï¼Œæ ¼å¼ä¸º

    UID=...; CID=...; SEID=...

ç„¶åè¿›å…¥è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œè¿è¡Œï¼ˆé»˜è®¤ç«¯å£ï¼š\x1b[1;36m80\x1b[0mï¼Œå¯ç”¨å‘½ä»¤è¡Œå‚æ•° \x1b[1m-p\x1b[0m/\x1b[1m--port\x1b[0m æŒ‡å®šå…¶å®ƒï¼‰

    python web_115_302_simple.py

æˆ–è€…ï¼ˆé»˜è®¤ç«¯å£ï¼š\x1b[1;36m8000\x1b[0mï¼Œå¯ç”¨å‘½ä»¤è¡Œå‚æ•° \x1b[1m--port\x1b[0m æŒ‡å®šå…¶å®ƒï¼‰

    uvicorn web_115_302_simple:app
""" % {
    "file_in_cwd": joinpath(realpath("."), "115-cookies.txt"), 
    "file_in_home": joinpath(realpath(expanduser("~")), "115-cookies.txt"), 
    "file_in_dir": joinpath(realpath(dirname(__file__)), "115-cookies.txt"), 
}

if __name__ == "__main__":
    from argparse import ArgumentParser, RawTextHelpFormatter

    parser = ArgumentParser(
        formatter_class=RawTextHelpFormatter, 
        description=__doc__, 
    )
    parser.add_argument("-H", "--host", default="0.0.0.0", help="ip æˆ– hostnameï¼Œé»˜è®¤å€¼ï¼š'0.0.0.0'")
    parser.add_argument("-p", "--port", default=80, type=int, help="ç«¯å£å·ï¼Œé»˜è®¤å€¼ï¼š80")
    parser.add_argument("-v", "--version", action="store_true", help="è¾“å‡ºç‰ˆæœ¬å·")

    args = parser.parse_args()
    if args.version:
        print(".".join(map(str, __version__)))
        raise SystemExit(0)

print(__doc__)

from os import environ, stat

cookies = bytes(environ.get("cookies", "").strip(), "latin-1")
cookies_path = environ.get("cookies_path", "")
cookies_path_mtime = 0
device = ""
path_persistence_commitment = environ.get("path_persistence_commitment") is not None
cdn_image = environ.get("cdn_image") is not None
cdn_image_warmup_ids = environ.get("cdn_image_warmup_ids", "")
cdn_image_warmup_no_path_cache = environ.get("cdn_image_warmup_no_path_cache") is not None
url_ttl = float(environ.get("url_ttl", "1"))
url_reuse_factor = int(environ.get("url_reuse_factor", "-1"))
url_range_request_cooldown = int(environ.get("url_range_request_cooldown", "0"))

if not cookies:
    if cookies_path:
        try:
            cookies = open(cookies_path, "rb").read().strip()
            cookies_path_mtime = stat(cookies_path).st_mtime_ns
        except FileNotFoundError:
            pass
    else:
        seen = set()
        for cookies_dir in (".", expanduser("~"), dirname(__file__)):
            cookies_dir = realpath(cookies_dir)
            if cookies_dir in seen:
                continue
            seen.add(cookies_dir)
            try:
                path = joinpath(cookies_dir, "115-cookies.txt")
                if cookies := open(path, "rb").read().strip():
                    cookies_path = path
                    cookies_path_mtime = stat(cookies_path).st_mtime_ns
                    break
            except FileNotFoundError:
                pass
if not cookies:
    raise SystemExit("unable to get cookies")


import errno
import logging

from asyncio import create_task, sleep, Lock
from collections.abc import Iterable, Iterator, MutableMapping
try:
    from collections.abc import Buffer # type: ignore
except ImportError:
    Buffer = bytes | bytearray | memoryview
from base64 import b64decode, b64encode
from enum import Enum
from functools import partial, update_wrapper
from posixpath import split as splitpath
from time import time
from typing import cast, Final
from urllib.parse import urlencode, urlsplit

try:
    import blacksheep
    from blacksheep import Application, route, redirect, text
    from blacksheep.client.session import ClientSession
    from blacksheep.common.types import normalize_headers
    from blacksheep.contents import FormContent
    from blacksheep.exceptions import HTTPException
    from blacksheep.server.remotes.forwarding import ForwardedHeadersMiddleware
    from blacksheep.messages import Request, Response
    from cachetools import LRUCache, TTLCache
    from orjson import dumps, loads
    from p115cipher import rsa_encode, rsa_decode
except ImportError:
    from sys import executable
    from subprocess import run
    run([executable, "-m", "pip", "install", "-U", *__requirements__], check=True)
    import blacksheep
    from blacksheep import Application, route, redirect, text
    from blacksheep.client.session import ClientSession
    from blacksheep.common.types import normalize_headers
    from blacksheep.contents import FormContent
    from blacksheep.exceptions import HTTPException
    from blacksheep.server.remotes.forwarding import ForwardedHeadersMiddleware
    from blacksheep.messages import Request, Response
    from cachetools import LRUCache, TTLCache
    from orjson import dumps, loads
    from p115cipher import rsa_encode, rsa_decode


# TODO: æŠŠå„ç§å·¥å…·æ”¾å…¥å‡½æ•°ï¼Œä¸è¦æ˜¯å…¨å±€å˜é‡
# TODO: è¿™ä¸ªå·¥å…·ï¼Œé›†æˆåˆ° p115 ä¸­

app = Application()
logger = getattr(app, "logger")
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("[\x1b[1m%(asctime)s\x1b[0m] (\x1b[1;36m%(levelname)s\x1b[0m) \x1b[5;31mâœ\x1b[0m %(message)s"))
logger.addHandler(handler)
cookies_lock = Lock()

# NOTE: id åˆ° pickcode çš„æ˜ å°„
ID_TO_PICKCODE: MutableMapping[str, str] = LRUCache(65536)
# NOTE: sha1 åˆ° pickcode åˆ°æ˜ å°„
SHA1_TO_PICKCODE: MutableMapping[str, str] = LRUCache(65536)
# NOTE: è·¯å¾„åˆ° id åˆ°æ˜ å°„
PATH_TO_ID: MutableMapping[str, str] = LRUCache(1048576 if path_persistence_commitment else 65536)
# NOTE: é“¾æ¥ç¼“å­˜ï¼Œå¦‚æœæ”¹æˆ Noneï¼Œåˆ™ä¸ç¼“å­˜ï¼Œå¯ä»¥è‡ªè¡Œè®¾å®š ttl (time-to-live)
URL_CACHE: None | MutableMapping[tuple[str, str], tuple[str, int]] = None
if url_reuse_factor not in (0, 1):
    if url_ttl > 0:
        URL_CACHE = TTLCache(1024, ttl=url_ttl)
    elif url_ttl < 0:
        URL_CACHE = LRUCache(1024)
# NOTE: ç¼“å­˜å›¾ç‰‡çš„ CDN ç›´é“¾ 1 å°æ—¶
IMAGE_URL_CACHE: MutableMapping[str, bytes] = TTLCache(float("inf"), ttl=3600)
# NOTE: æ¯ä¸ª ip å¯¹äºæŸä¸ªèµ„æºçš„æŸä¸ª range è¯·æ±‚ï¼Œä¸€å®šæ—¶é—´èŒƒå›´å†…ï¼Œåˆ†åˆ«åªæ”¾è¡Œä¸€ä¸ªï¼Œå¯ä»¥è‡ªè¡Œè®¾å®š ttl (time-to-live)
RANGE_REQUEST_COOLDOWN: None | MutableMapping[tuple[str, str, str, bytes], None] = None
if url_range_request_cooldown > 0:
    RANGE_REQUEST_COOLDOWN = TTLCache(8196, ttl=url_range_request_cooldown)


# TODO: ç™»å½•ä½¿ç”¨å•ç‹¬çš„æ¨¡å—ï¼Œå¦å¤–ä¸¤ä¸ª qrcode_cookie*.py çš„æ–‡ä»¶è¦è¢«åˆ æ‰
# TODO: å®ç°åŒæ­¥å’Œå¼‚æ­¥çš„ç‰ˆæœ¬
AppEnum = Enum("AppEnum", {
    "web": 1, 
    "ios": 6, 
    "115ios": 8, 
    "android": 9, 
    "115android": 11, 
    "115ipad": 14, 
    "tv": 15, 
    "qandroid": 16, 
    "windows": 19, 
    "mac": 20, 
    "linux": 21, 
    "wechatmini": 22, 
    "alipaymini": 23, 
})


def get_enum_name(val, cls):
    if isinstance(val, cls):
        return val.name
    try:
        if isinstance(val, str):
            return cls[val].name
    except KeyError:
        pass
    return cls(val).name


def redirect_exception_response(func, /):
    async def wrapper(*args, **kwds):
        try:
            return await func(*args, **kwds)
        except BaseException as e:
            message = f"{type(e).__module__}.{type(e).__qualname__}: {e}"
            logger.error(message)
            if isinstance(e, HTTPException):
                return text(message, e.status)
            elif isinstance(e, AuthenticationError):
                return text(str(e), 401)
            elif isinstance(e, PermissionError):
                return text(str(e), 403)
            elif isinstance(e, FileNotFoundError):
                return text(str(e), 404)
            elif isinstance(e, (IsADirectoryError, NotADirectoryError)):
                return text(str(e), 406)
            elif isinstance(e, OSError):
                return text(str(e), 500)
            elif isinstance(e, Exception):
                return text(str(e), 503)
            raise
    return update_wrapper(wrapper, func)


async def do_request(
    client: ClientSession, 
    url: str | bytes | blacksheep.url.URL, 
    method: str = "GET", 
    content: None | blacksheep.contents.Content = None, 
    headers: None | dict[str, str] = None, 
    params: None | dict[str, str] = None, 
) -> Response:
    global cookies, cookies_path_mtime
    request_headers: list[tuple[bytes, bytes]]
    if headers:
        request_headers = normalize_headers(headers) # type: ignore
    else:
        request_headers = []
    current_cookies = cookies
    request_headers.append((b"Cookie", current_cookies))
    request = Request(method.upper(), client.get_url(url, params), request_headers)
    response = await client.send(request.with_content(content) if content else request)
    if response.status == 405:
        async with cookies_lock:
            if cookies_path:
                try:
                    if cookies_path_mtime != stat(cookies_path).st_mtime_ns:
                        cookies = open(cookies_path, "rb").read().strip()
                        cookies_path_mtime = stat(cookies_path).st_mtime_ns
                except FileNotFoundError:
                    pass
            if current_cookies == cookies:
                await relogin(client)
        return await do_request(client, url, method, content, headers, params)
    if response.status >= 400:
        raise HTTPException(response.status, response.reason)
    return response


async def request_json(
    client: ClientSession, 
    url: str | bytes | blacksheep.url.URL, 
    method: str = "GET", 
    content: None | blacksheep.contents.Content = None, 
    headers: None | dict[str, str] = None, 
    params: None | dict[str, str] = None, 
) -> dict:
    global cookies, cookies_path_mtime
    current_cookies = cookies
    resp = await do_request(client, url, method, content=content, headers=headers, params=params)
    json = loads((await resp.read()) or b"")
    try:
        return check_response(json)
    except AuthenticationError:
        async with cookies_lock:
            if cookies_path:
                try:
                    if cookies_path_mtime != stat(cookies_path).st_mtime_ns:
                        cookies = open(cookies_path, "rb").read().strip()
                        cookies_path_mtime = stat(cookies_path).st_mtime_ns
                except FileNotFoundError:
                    pass
            if current_cookies == cookies:
                raise
        return await request_json(client, url, method, content=content, headers=headers, params=params)


# TODO: ä¸éœ€è¦æ­¤æ¥å£ï¼Œç›´æ¥æ ¹æ® user_id çš„ ssoent æ¥åˆ¤æ–­
async def login_device(client: ClientSession) -> str:
    url = "https://passportapi.115.com/app/1.0/web/1.0/login_log/login_devices"
    resp = await request_json(client, url)
    return next((d["icon"] for d in resp["data"]["list"] if d["is_current"]), "qandroid")


async def login_qrcode_token(client: ClientSession) -> dict:
    """è·å–äºŒç»´ç 
    """
    url = "https://qrcodeapi.115.com/api/1.0/web/1.0/token/"
    return await request_json(client, url)


async def login_qrcode_scan(client: ClientSession, uid: str) -> dict:
    """æ‰«æäºŒç»´ç 
    """
    url = f"https://qrcodeapi.115.com/api/2.0/prompt.php"
    return await request_json(client, url, params={"uid": uid})


async def login_qrcode_scan_confirm(client: ClientSession, uid: str) -> dict:
    """ç¡®è®¤æ‰«æäºŒç»´ç 
    """
    url = f"https://hnqrcodeapi.115.com/api/2.0/slogin.php"
    return await request_json(client, url, params={"key": uid, "uid": uid, "client": "0"})


async def login_qrcode_scan_result(client: ClientSession, uid: str, app: str = "web") -> dict:
    """æŠŠæ‰«ç ç»“æœç»‘å®šåˆ°è®¾å¤‡
    """
    app = get_enum_name(app, AppEnum)
    url = "https://passportapi.115.com/app/1.0/%s/1.0/login/qrcode/" % app
    return await request_json(client, url, "POST", content=FormContent({"account": uid}))


async def relogin(client: ClientSession) -> dict:
    """è‡ªåŠ¨æ‰«äºŒç»´ç é‡æ–°ç™»å½•
    """
    global cookies, cookies_path_mtime, device
    if not device:
        device = await login_device(client)
    logger.warning(f"\x1b[1m\x1b[33m[SCAN] ğŸ¦¾ é‡æ–°æ‰«ç : {device!r} ğŸ¦¿\x1b[0m")
    uid = (await login_qrcode_token(client))["data"]["uid"]
    await login_qrcode_scan(client, uid)
    await login_qrcode_scan_confirm(client, uid)
    resp = await login_qrcode_scan_result(client, uid, device)
    cookies = bytes("; ".join("%s=%s" % e for e in resp["data"]["cookie"].items()), "latin-1")
    if cookies_path:
        open(cookies_path, "wb").write(cookies)
        cookies_path_mtime = stat(cookies_path).st_mtime_ns
    return resp


# TODO: ä¸éœ€è¦ä¼ å…¥ dirï¼Œä½†æœ‰å…¨å±€çš„ id_to_dirï¼Œå¯ä»¥è‡ªåŠ¨ç¡®å®šè·¯å¾„
def process_info(info: dict, dir: None | str = None) -> str:
    if "file_id" in info:
        file_id = cast(str, info["file_id"])
        file_name = cast(str, info["file_name"])
        pick_code = cast(str, info["pick_code"])
        thumb = info.get("img_url", "")
        if "sha1" in info:
            SHA1_TO_PICKCODE[info["sha1"]] = pick_code
    else:
        file_id = cast(str, info["fid"])
        file_name = cast(str, info["n"])
        pick_code = cast(str, info["pc"])
        thumb = info.get("u", "")
        SHA1_TO_PICKCODE[info["sha"]] = pick_code
    ID_TO_PICKCODE[file_id] = pick_code
    if cdn_image and thumb:
        IMAGE_URL_CACHE[pick_code] = bytes(reduce_image_url_layers(thumb), "utf-8")
    if dir:
        PATH_TO_ID[dir + "/" + file_name] = file_id
    elif dir is not None:
        PATH_TO_ID[file_name] = file_id
    return pick_code


@app.on_middlewares_configuration
def configure_forwarded_headers(app):
    app.middlewares.insert(0, ForwardedHeadersMiddleware(accept_only_proxied_requests=False))


@app.lifespan
async def register_http_client():
    async with ClientSession(follow_redirects=False) as client:
        app.services.register(ClientSession, instance=client)
        yield


async def get_dir_patht_by_id(
    client: ClientSession, 
    id: str, 
    /, 
) -> list[tuple[str, str]]:
    json = await request_json(
        client, 
        "https://webapi.115.com/files", 
        params={
            "count_folders": "0", "record_open_time": "0", "show_dir": "1", 
            "cid": id, "limit": "1", "offset": "0", 
        }, 
    )
    return [(info["cid"], info["name"]) for info in json["path"][1:]]


async def get_attr(id: int, /):
    ...



# TODO: è¿™ä¸ªå‡½æ•°éœ€è¦è¿›è¡Œä¼˜åŒ–
async def get_pickcode_by_id(
    client: ClientSession, 
    id: str, 
    /, 
) -> str:
    if pickcode := ID_TO_PICKCODE.get(id):
        return pickcode
    json = await request_json(
        client, 
        "https://webapi.115.com/files/get_info", 
        params={"file_id": id}, 
    )
    info = json["data"][0]
    if "fid" not in info:
        raise FileNotFoundError(errno.ENOENT, id)
    return process_info(info)


async def get_pickcode_by_sha1(
    client: ClientSession, 
    sha1: str, 
    /, 
) -> str:
    if len(sha1) != 40:
        raise ValueError(f"invalid sha1 {sha1!r}")
    if pickcode := SHA1_TO_PICKCODE.get(sha1):
        return pickcode
    json = await request_json(
        client, 
        "https://webapi.115.com/files/shasearch", 
        params={"sha1": sha1}, 
    )
    if not json["state"]:
        raise FileNotFoundError(errno.ENOENT, f"no such sha1 {sha1!r}")
    return process_info(json["data"])


async def get_pickcode_by_path(
    client: ClientSession, 
    path: str, 
    disable_pc: bool = False, 
    /, 
) -> str:
    path = path.strip("/")
    dir_, name = splitpath(path)
    if not name:
        raise FileNotFoundError(path)
    if fid := PATH_TO_ID.get(path):
        if not disable_pc and path_persistence_commitment and (pickcode := ID_TO_PICKCODE.get(fid)):
            return pickcode
        json = await request_json(
            client, 
            "https://webapi.115.com/files/file", 
            params={"file_id": fid}, 
        )
        if json["state"]:
            info = json["data"][0]
            if info["file_name"] == name:
                return info["pick_code"]
        PATH_TO_ID.pop(path, None)
    if dir_:
        json = await request_json(
            client, 
            "https://webapi.115.com/files/getid", 
            params={"path": dir_}, 
        )
        if not (pid := json["id"]):
            raise FileNotFoundError(path)
    else:
        pid = 0
    # ä½¿ç”¨ iterdir æ–¹æ³•
    params = {"count_folders": 0, "record_open_time": 0, "show_dir": 1, "cid": pid, "limit": 10_000, "offset": 0}
    while True:
        json = await request_json(
            client, 
            "https://webapi.115.com/files", 
            params=params, 
        )
        it = iter(json["data"])
        for info in it:
            if "fid" in info:
                pickcode = process_info(info, dir_)
                if info["n"] == name:
                    for info in it:
                        process_info(info, dir_)
                    return pickcode
        if json["offset"] + len(json["data"]) == json["count"]:
            break
        params["offset"] += 5000
    raise FileNotFoundError(path)


def reduce_image_url_layers(url: str) -> str:
    if not url.startswith(("http://thumb.115.com/", "https://thumb.115.com/")):
        return url
    urlp = urlsplit(url)
    sha1 = urlp.path.rsplit("/")[-1].split("_")[0]
    return f"https://imgjump.115.com/?sha1={sha1}&{urlp.query}&size=0"


async def iterdir():
    ...

async def iter_files(
    client: ClientSession, 
    cid: str = "0", 
    /, 
) -> AsyncIterator[dict]:
    api = "https://webapi.115.com/files"
    payload: dict = {
        "aid": 1, "asc": 1, "cid": cid, "count_folders": 0, "cur": 0, "custom_order": 1, 
        "limit": 10_000, "o": "user_ptime", "offset": 0, "show_dir": 0, 
    }
    ...


# TODO: è¿™ä¸ªå‡½æ•°çš„ä»£ç ä¸è¯¥è¿™ä¹ˆå¤š
async def warmup_cdn_image(
    client: ClientSession, 
    cid: str = "0", 
    /, 
    cache: None | dict[str, str] = None, 
) -> int:
    api = "https://webapi.115.com/files"
    payload: dict = {
        "aid": 1, "asc": 1, "cid": cid, "count_folders": 0, "cur": 0, "custom_order": 1, 
        "limit": 10_000, "o": "user_ptime", "offset": 0, "show_dir": 0, "type": 2, 
    }
    count = 0
    while True:
        resp = await request_json(client, api, params=payload)
        for item in resp["data"]:
            # TODO: ä½¿ç”¨ process_infoï¼Œæ”¹åä¸º normalize_info
            file_id = item["file_id"]
            pickcode = item["pick_code"]
            IMAGE_URL_CACHE[pickcode] = bytes(reduce_image_url_layers(item["thumb_url"]), "utf-8")
            ID_TO_PICKCODE[file_id] = pickcode
            SHA1_TO_PICKCODE[item["sha1"]] = pickcode
            if cache is not None:
                parent_id = str(item["parent_id"])
                dirname = ""
                if parent_id != "0" and not (dirname := cache.get(parent_id, "")):
                    patht = await get_dir_patht_by_id(client, parent_id)
                    for pid, name in patht:
                        if dirname:
                            dirname += "/" + name
                        else:
                            dirname = name
                        cache[pid] = dirname
                path = item["file_name"]
                if dirname:
                    path = dirname + "/" + path
                PATH_TO_ID[path] = file_id
        total = resp["count"]
        delta = len(resp["data"])
        count += delta
        logger.info("successfully cached %s (finished=%s, total=%s) cdn images in %s", delta, count, total, id)
        if count >= total:
            break
        payload["offset"] += 10_000
    return count


if cdn_image and cdn_image_warmup_ids:
    async def periodically_warmup_cdn_image(client: ClientSession, ids: str):
        id_list = [int(id) for id in ids.split(",") if id]
        if not id_list:
            return
        cache: None | dict[str, str] = None
        if not cdn_image_warmup_no_path_cache:
            cache = {}
        while True:
            start = time()
            for id in map(str, id_list):
                if cache and id in cache:
                    logger.warning("skipped cdn images warmup-ing in %s", id)
                    continue
                logger.info("background task start: warmup-ing cdn images in %s", id)
                try:
                    count = await warmup_cdn_image(client, id, cache=cache)
                except Exception:
                    logger.exception("error occurred while warmup-ing cdn images in %s", id)
                else:
                    logger.info("background task stop: warmup-ed cdn images in %s, count=%s", id, count)
            if (interval := start + 3600 - time()) > 0:
                await sleep(interval)

    @app.on_start
    async def configure_background_tasks(app: Application):
        client = app.services.resolve(ClientSession)
        create_task(periodically_warmup_cdn_image(client, cdn_image_warmup_ids))


# TODO: å¦‚æœéœ€è¦æ ¹æ®æ–‡ä»¶ id è·å–åŸºæœ¬çš„ä¿¡æ¯ï¼Œå¯ä»¥ç”¨ fs_file_skimï¼ˆå¯ä»¥ä¸€æ¬¡æŸ¥å¤šä¸ªï¼‰ï¼Œå¦‚æœå¯èƒ½è¿˜éœ€è¦å›¾ç‰‡é“¾æ¥ï¼Œåˆ™ç”¨fs_info
# TODO: å¯ä»¥æ˜¯ id ä¹Ÿå¯ä»¥æ˜¯ pickcodeï¼ˆä¸ºäº†åŠ é€Ÿï¼‰
# è¿™ä¸ªæ¥å£æœ‰å¤šä¸ªä¿¡æ¯å¯ç”¨ï¼ˆpick_code,file_sha1ï¼Œä½†æ— idï¼‰
async def get_image_url(
    client: ClientSession, 
    pickcode: str, 
) -> bytes:
    """è·å–å›¾ç‰‡çš„ cdn é“¾æ¥
    """
    if IMAGE_URL_CACHE and (url := IMAGE_URL_CACHE.get(pickcode)):
        return url
    json = await request_json(
        client, 
        "https://webapi.115.com/files/image", 
        params={"pickcode": pickcode}, 
    )
    origin_url = json["data"]["origin_url"]
    resp = await do_request(client, origin_url, "HEAD")
    url = cast(bytes, resp.get_first_header(b"Location"))
    if IMAGE_URL_CACHE is not None:
        IMAGE_URL_CACHE[pickcode] = url
    return url


# TODO è¿™ä¸ªå‡½æ•°éœ€è¦å¤§å¤§æ‹†åˆ†ï¼Œè¿›è¡Œå·¨å¤§çš„ç®€åŒ–
@route("/", methods=["GET", "HEAD"])
@route("/{path:path2}", methods=["GET", "HEAD"])
@redirect_exception_response
async def get_download_url(
    request: Request, 
    client: ClientSession, 
    pickcode: str = "", 
    id: str = "", 
    sha1: str = "", 
    path: str = "", 
    path2: str = "", 
    image: bool = False, 
    disable_pc: bool = False, 
):
    """è·å–æ–‡ä»¶çš„ä¸‹è½½é“¾æ¥

    :param pickcode: æ–‡ä»¶æˆ–ç›®å½•çš„ pickcodeï¼Œä¼˜å…ˆçº§é«˜äº id
    :param id: æ–‡ä»¶çš„ idï¼Œä¼˜å…ˆçº§é«˜äº sha1
    :param sha1: æ–‡ä»¶çš„ sha1ï¼Œä¼˜å…ˆçº§é«˜äº path
    :param path: æ–‡ä»¶çš„è·¯å¾„ï¼Œä¼˜å…ˆçº§é«˜äº path2
    :param path2: æ–‡ä»¶çš„è·¯å¾„ï¼Œè¿™ä¸ªç›´æ¥åœ¨æ¥å£è·¯å¾„ä¹‹åï¼Œä¸åœ¨æŸ¥è¯¢å­—ç¬¦ä¸²ä¸­
    :param image: è§†ä¸ºå›¾ç‰‡ï¼ˆå½“æä¾› pickcode ä¸”è®¾ç½®äº†ç¯å¢ƒå˜é‡ cdn_imageï¼‰
    :param disable_pc: è§† path_persistence_commitment ä¸º False
    """
    try:
        user_agent = (request.get_first_header(b"User-agent") or b"").decode("utf-8")
        if not (pickcode := pickcode.strip()):
            if id := id.strip():
                pickcode = await get_pickcode_by_id(client, id)
            elif sha1 := sha1.strip():
                pickcode = await get_pickcode_by_sha1(client, sha1)
            else:
                pickcode = await get_pickcode_by_path(client, path or path2)
        if RANGE_REQUEST_COOLDOWN is not None:
            key = (request.client_ip or "", user_agent, pickcode, request.get_first_header(b"Range") or b"")
            if key in RANGE_REQUEST_COOLDOWN:
                return text("Too Many Requests", 429)
            RANGE_REQUEST_COOLDOWN[key] = None
        if URL_CACHE is not None and (t := URL_CACHE.get((pickcode, user_agent))):
            url, times = t
            if url_reuse_factor < 0 or times < url_reuse_factor:
                URL_CACHE[(pickcode, user_agent)] = (url, times + 1)
                return redirect(url)
        if cdn_image and (image or pickcode in IMAGE_URL_CACHE):
            return redirect(await get_image_url(client, pickcode))
        # TODO: éœ€è¦å•ç‹¬å°è£…
        json = await request_json(
            client, 
            "https://proapi.115.com/app/chrome/downurl", 
            method="POST", 
            content=FormContent({"data": rsa_encode(b'{"pickcode":"%s"}' % bytes(pickcode, "ascii")).decode("ascii")}), 
            headers={"User-Agent": user_agent}, 
        )
        data = loads(rsa_decode(json["data"]))
        item = next(info for info in data.values())
        ID_TO_PICKCODE[next(iter(data))] = item["pick_code"]
        # NOTE: è¿˜éœ€è¦ç»§ç»­å¢åŠ ï¼Œç›®å‰ä¸ç¡®å®š 115 åˆ°åº•æ”¯æŒå“ªäº›å›¾ç‰‡æ ¼å¼
        if cdn_image and item["file_name"].lower().endswith((
            ".bmp", ".gif", ".heic", ".heif", ".jpeg", ".jpg", ".png", ".raw", ".svg", ".tif", ".tiff", ".webp", 
        )):
            IMAGE_URL_CACHE[item["pick_code"]] = "" # type: ignore
        url = item["url"]["url"]
        if URL_CACHE is not None:
            URL_CACHE[(pickcode, user_agent)] = (url, 1)
        return redirect(cast(str, url))
    except (FileNotFoundError, KeyError):
        return text("not found", 404) 


if __name__ == "__main__":
    try:
        import uvicorn
    except ImportError:
        from sys import executable
        from subprocess import run
        run([executable, "-m", "pip", "install", "-U", "uvicorn"], check=True)
        import uvicorn
    uvicorn.run(
        app=app, 
        host=args.host, 
        port=args.port, 
        proxy_headers=True, 
        forwarded_allow_ips="*", 
    )

# TODO ä½œä¸ºæ¨¡å—æä¾›ï¼Œè¿”å›ä¸€ä¸ª app å¯¹è±¡ï¼Œä»¥ä¾¿å’Œå…¶å®ƒæ¨¡å—é›†æˆ
# TODO æ¢ä¸ªæ¡†æ¶ robynï¼Ÿ
# TODO åŒæ­¥æ¡†æ¶é€‰ç”¨ flaskï¼Œå¼‚æ­¥æ¡†æ¶è¿˜è¦å†æŒ‘ä¸€æŒ‘
# TODO ä¸ webdav é›†æˆï¼ˆå¯ä»¥å…³é—­ï¼‰
# TODO åº”è¯¥ä½œä¸ºå•ç‹¬æ¨¡å—æä¾›ï¼ˆä»¥ä¾¿å’Œå…¶å®ƒé¡¹ç›®é›†æˆï¼‰ï¼Œæäº¤åˆ° pypiï¼Œåå­—å« p115302ï¼Œæä¾›åŒæ­¥å’Œå¼‚æ­¥çš„ç‰ˆæœ¬ï¼Œä½†ä¸ä¾èµ–äºp115
# TODO ä»»ä½•æ¥å£éƒ½è¦æœ‰ä¸€ä¸ªå•ç‹¬çš„å°è£…å‡½æ•°
# TODO å„ç§å‡½æ•°éƒ½è¦ç®€åŒ–æˆ–è€…æ‹†åˆ†
# TODO æŸ¥è¯¢sha1ç”¨æ–°çš„æ¥å£
# TODO å¦‚æœå›¾ç‰‡éœ€è¦è·¯å¾„ï¼Œåˆ™ç”¨æ‰¹é‡æ‰“æ˜Ÿæ ‡çš„åŠæ³•å®ç°
# TODO ç¼“å­˜ id_to_dir
# TODO æ›´å¥½çš„ç®—æ³•ï¼Œä»¥å¿«é€Ÿæ›´æ–° PATH_TO_ID
# TODO è¿™ä¸ªæ–‡ä»¶å¯ä»¥å®ç°ä¸ºä¸€ä¸ªæ¨¡å—
# TODO ä¸éœ€è¦åˆ¤æ–­ login_device
# TODO å¯ä»¥ä¸ºå¤šç§ç±»å‹çš„æ–‡ä»¶é¢„çƒ­ï¼ˆä¾‹å¦‚å›¾ç‰‡æˆ–è§†é¢‘ï¼‰
# TODO å…è®¸å¯¹é“¾æ¥è¿›è¡Œç­¾åï¼šå‘½ä»¤è¡Œä¼ å…¥tokenï¼ˆæœ‰tokenæ—¶æ‰åšç­¾åï¼‰ï¼Œé“¾æ¥é‡Œå¯ä»¥åŒ…å«æˆªæ­¢æ—¶é—´ï¼ˆé»˜è®¤ä¸º0ï¼Œå³æ°¸ä¸å¤±æ•ˆï¼‰ï¼Œç„¶åç”± f"302@115-{t}-{value}#{type}-{token}" t æ˜¯æˆªæ­¢æ—¶é—´ï¼Œåé¢çš„ type æ˜¯ç±»å‹ï¼ŒåŒ…æ‹¬sha1,pickcode,path,idï¼Œå†è®¡ç®—ä¸€ä¸‹å“ˆå¸Œ


#!/usr/bin/env python3
# coding: utf-8

__author__ = "ChenyangGao <https://chenyanggao.github.io>"
__doc__ = "ç›®å½•æ ‘ä¿¡æ¯éå†å¯¼å‡º"

from argparse import ArgumentParser, Namespace, RawTextHelpFormatter
from hashlib import algorithms_available

KEYS = ["inode", "name", "path", "relpath", "is_dir", "stat", "stat_info"]

parser = ArgumentParser(description=__doc__, formatter_class=RawTextHelpFormatter)

parser.add_argument("path", nargs="?", default=".", help="æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰å·¥ä½œç›®å½•")
parser.add_argument("-m", "--min-depth", default=0, type=int, help="æœ€å°æ·±åº¦ï¼Œé»˜è®¤å€¼ 0ï¼Œå°äº 0 æ—¶ä¸é™")
parser.add_argument("-M", "--max-depth", default=-1, type=int, help="æœ€å¤§æ·±åº¦ï¼Œé»˜è®¤å€¼ -1ï¼Œå°äº 0 æ—¶ä¸é™")
parser.add_argument("-k", "--keys", choices=KEYS, nargs="*", help=f"é€‰æ‹©è¾“å‡ºçš„ keyï¼Œé»˜è®¤è¾“å‡ºæ‰€æœ‰å¯é€‰å€¼")
parser.add_argument("-s", "--select", default="", help="å¯¹è·¯å¾„è¿›è¡Œç­›é€‰ï¼Œæä¾›ä¸€ä¸ªè¡¨è¾¾å¼ï¼ˆä¼šæ³¨å…¥ä¸€ä¸ªå˜é‡ entryï¼Œç±»å‹æ˜¯ iterdir.DirEntryï¼‰æˆ–å‡½æ•°ï¼ˆä¼šä¼ å…¥ä¸€ä¸ªå‚æ•°ï¼Œç±»å‹æ˜¯ iterdir.DirEntryï¼‰")
parser.add_argument("-se", "--select-exec", action="store_true", help="å¯¹ -s/--select ä¼ å…¥çš„ä»£ç ç”¨ exec è¿è¡Œï¼Œå…¶ä¸­å¿…é¡»å­˜åœ¨åä¸º select çš„å‡½æ•°ã€‚å¦åˆ™ï¼Œè§†ä¸ºè¡¨è¾¾å¼æˆ– lambda å‡½æ•°")
parser.add_argument("-o", "--output-file", help="""ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ­¤æ—¶å‘½ä»¤è¡Œä¼šè¾“å‡ºè¿›åº¦æ¡ï¼Œæ ¹æ®æ‰©å±•åæ¥å†³å®šè¾“å‡ºæ ¼å¼
- *.csv   è¾“å‡ºä¸€ä¸ª csvï¼Œç¬¬ 1 è¡Œä¸ºè¡¨å¤´ï¼Œä»¥åæ¯è¡Œè¾“å‡ºä¸€æ¡æ•°æ®
- *.json  è¾“å‡ºä¸€ä¸ª JSON Object çš„åˆ—è¡¨
- *       æ¯è¡Œè¾“å‡ºä¸€æ¡ JSON Object
""")
parser.add_argument("-hs", "--hashes", choices=(*algorithms_available, "crc32"), nargs="*", help="è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªç®—æ³•")
parser.add_argument("-dfs", "--depth-first", action="store_true", help="ä½¿ç”¨æ·±åº¦ä¼˜å…ˆæœç´¢ï¼Œå¦åˆ™ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆ")
parser.add_argument("-fl", "--follow-symlinks", action="store_true", help="è·Ÿè¿›ç¬¦å·è¿æ¥ï¼Œå¦åˆ™ä¼šæŠŠç¬¦å·é“¾æ¥è§†ä¸ºæ–‡ä»¶ï¼Œå³ä½¿å®ƒæŒ‡å‘ç›®å½•")
parser.add_argument("-v", "--version", action="store_true", help="è¾“å‡ºç‰ˆæœ¬å·")

from binascii import crc32
from collections.abc import Callable, Iterator, Sequence
from functools import partial
from hashlib import new as hashnew
from os import fsdecode, PathLike
from os.path import abspath, isdir, islink, relpath
from sys import stdout
from textwrap import dedent
from typing import cast, Any, TextIO


def file_multi_hashes(
    path: bytes | str | PathLike, 
    hashes: Sequence[str], 
) -> None | dict[str, str]:
    try:
        file = open(path, "rb", buffering=0)
    except OSError:
        return None
    cache: dict[str, Any] = {alg: 0 if alg == "crc32" else hashnew(alg) for alg in hashes}
    updates = tuple(
        (lambda data: 
            cache.__setitem__("crc32", crc32(data, cast(int, cache["crc32"])))
        ) if alg == "crc32" else val.update 
        for alg, val in cache.items()
    )
    readinto = file.readinto
    buf = bytearray(1 << 16) # 64 KB
    view = memoryview(buf)
    while (size := readinto(buf)):
        for update in updates:
            update(view[:size])
    return {alg: format(val, "x") if alg == "crc32" else val.hexdigest() for alg, val in cache.items()}


def parse_args(argv: None | list[str] = None, /) -> Namespace:
    args = parser.parse_args(argv)
    if args.version:
        from iterdir import __version__
        print(".".join(map(str, __version__)))
        raise SystemExit(0)
    return args


def main(argv: None | list[str] | Namespace = None, /):
    if isinstance(argv, Namespace):
        args = argv
    else:
        args = parse_args(argv)

    from iterdir import iterdir, DirEntry

    predicate: None | Callable[[DirEntry], None | bool] = None
    if select_code := dedent(args.select).strip():
        ns: dict = {"re": __import__("re")}
        if args.select_exec:
            exec(select_code, ns)
            predicate = ns.get("select")
        elif select_code.startswith("lambda "):
            predicate = eval(select_code, ns)
        else:
            predicate = eval("lambda entry:" + select_code, ns)

    follow_symlinks = args.follow_symlinks
    path_it: Iterator[DirEntry] = iterdir(
        args.path, 
        topdown=True if args.depth_first else None, 
        min_depth=args.min_depth, 
        max_depth=args.max_depth, 
        predicate=predicate, 
        follow_symlinks=follow_symlinks, 
    )

    output_file = args.output_file
    if output_file:
        from collections import deque
        from time import perf_counter
        from texttools import format_time

        def progress(it):
            write = stdout.buffer.raw.write # type: ignore
            dq: deque[tuple[int, float]] = deque(maxlen=10*60)
            push = dq.append
            total = 0
            ndirs = 0
            nfiles = 0
            start_t = last_t = perf_counter()
            write(f"\r\x1b[KğŸ—‚ï¸  {total} = ğŸ“‚ {ndirs} + ğŸ“ {nfiles}".encode())
            push((total, start_t))
            for p in it:
                total += 1
                if p.is_dir():
                    ndirs += 1
                else:
                    nfiles += 1
                cur_t = perf_counter()
                if cur_t - last_t > 0.1:
                    speed = (total - dq[0][0]) / (cur_t - dq[0][1])
                    write(f"\r\x1b[KğŸ—‚ï¸  {total} = ğŸ“‚ {ndirs} + ğŸ“ {nfiles} | ğŸ•™ {format_time(cur_t-start_t)} | ğŸš€ {speed:.3f} it/s".encode())
                    push((total, cur_t))
                    last_t = cur_t
                yield p
            cur_t = perf_counter()
            speed = total / (cur_t - start_t)
            write(f"\r\x1b[KğŸ—‚ï¸  {total} = ğŸ“‚ {ndirs} + ğŸ“ {nfiles} | ğŸ•™ {format_time(cur_t-start_t)} | ğŸš€ {speed:.3f} it/s".encode())
        file: TextIO = open(output_file, "w")
        path_it = iter(progress(path_it))
        if output_file.endswith(".csv"):
            output_type = "csv"
        elif output_file.endswith(".json"):
            output_type = "json"
        else:
            output_type = "log"
    else:
        file = stdout
        output_type = "log"
    write = file.buffer.write
    if output_type in ("log", "json"):
        from orjson import dumps

    fmap: dict[str, Callable] = {
        "inode": DirEntry.inode, 
        "name": lambda e: e.name, 
        "path": lambda e: e.path, 
        "relpath": lambda e, start=abspath(args.path): relpath(abspath(e), start), 
        "is_dir": lambda e: e.is_dir(follow_symlinks=follow_symlinks), 
        "stat": lambda e: e.stat_dict(follow_symlinks=follow_symlinks, with_st=True), 
        "stat_info": lambda e: e.stat_info(follow_symlinks=follow_symlinks), 
    }

    keys: list[str] = args.keys
    if keys:
        fmap = {k: fmap[k] for k in keys if k in fmap}
        keys = list(fmap)
    else:
        keys = KEYS

    if args.hashes:
        keys.append("hashes")
        fmap["hashes"] = partial(file_multi_hashes, hashes=args.hashes)

    try:
        records = ({k: f(e) for k, f in fmap.items()} for e in path_it)
        if output_type == "json":
            write(b"[")
            for i, record in enumerate(records):
                if i:
                    write(b", ")
                write(dumps(record))
            write(b"]")
        elif output_type == "log":
            for record in records:
                write(dumps(record))
                write(b"\n")
        else:
            from csv import DictWriter

            writer = DictWriter(file, fieldnames=keys)
            writer.writeheader()
            for record in records:
                writer.writerow(record)
    except KeyboardInterrupt:
        pass
    except BrokenPipeError:
        from sys import stderr
        stderr.close()
    finally:
        file.close()


if __name__ == "__main__":
    from pathlib import Path
    from sys import path

    path[0] = str(Path(__file__).parents[1])
    main()


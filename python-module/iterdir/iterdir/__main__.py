#!/usr/bin/env python3
# coding: utf-8

__author__ = "ChenyangGao <https://chenyanggao.github.io>"
__doc__ = "ç›®å½•æ ‘ä¿¡æ¯éå†å¯¼å‡º"

from argparse import ArgumentParser, RawTextHelpFormatter
from hashlib import algorithms_available

KEYS = ["inode", "name", "path", "relpath", "isdir", "islink", "stat"]

parser = ArgumentParser(description=__doc__, formatter_class=RawTextHelpFormatter)

parser.add_argument("path", nargs="?", default="", help="æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰å·¥ä½œç›®å½•")
parser.add_argument("-m", "--min-depth", default=0, type=int, help="æœ€å°æ·±åº¦ï¼Œé»˜è®¤å€¼ 0ï¼Œå°äº 0 æ—¶ä¸é™")
parser.add_argument("-M", "--max-depth", default=-1, type=int, help="æœ€å¤§æ·±åº¦ï¼Œé»˜è®¤å€¼ -1ï¼Œå°äº 0 æ—¶ä¸é™")
parser.add_argument("-k", "--keys", choices=KEYS, nargs="*", help=f"é€‰æ‹©è¾“å‡ºçš„ keyï¼Œé»˜è®¤è¾“å‡ºæ‰€æœ‰å¯é€‰å€¼")
parser.add_argument("-t", "--output-type", choices=("log", "json", "csv"), default="log", help="""\
è¾“å‡ºç±»å‹ï¼Œé»˜è®¤ä¸º log
- log   æ¯è¡Œè¾“å‡ºä¸€æ¡æ•°æ®ï¼Œæ¯æ¡æ•°æ®è¾“å‡ºä¸ºä¸€ä¸ª json çš„ object
- json  è¾“å‡ºä¸€ä¸ª json çš„ listï¼Œæ¯æ¡æ•°æ®è¾“å‡ºä¸ºä¸€ä¸ª json çš„ object
- csv   è¾“å‡ºä¸€ä¸ª csvï¼Œç¬¬ 1 è¡Œä¸ºè¡¨å¤´ï¼Œä»¥åæ¯è¡Œè¾“å‡ºä¸€æ¡æ•°æ®
""")
parser.add_argument("-d", "--dump", default="", help="""\
(ä¼˜å…ˆçº§é«˜äº -k/--keysã€-hs/--hashesã€-t/--output-type) è°ƒç”¨ä»¥å¯¼å‡ºæ•°æ®ï¼Œå¦‚æœæœ‰è¿”å›å€¼åˆ™å†è¡Œè¾“å‡ºï¼Œå°¾éƒ¨ä¼šæ·»åŠ ä¸€ä¸ª b'\n'ã€‚
å¦‚æœç»“æœ result æ˜¯
    - Noneï¼Œè·³è¿‡
    - bytesï¼Œè¾“å‡º
    - å…¶å®ƒï¼Œå…ˆè°ƒç”¨ `bytes(str(result), 'utf-8')`ï¼Œå†è¾“å‡º
æä¾›ä¸€ä¸ªè¡¨è¾¾å¼ï¼ˆä¼šæ³¨å…¥ä¸€ä¸ªå˜é‡ pathï¼Œç±»å‹æ˜¯ pathlib.Pathï¼‰æˆ–å‡½æ•°ï¼ˆä¼šä¼ å…¥ä¸€ä¸ªå‚æ•°ï¼Œç±»å‹æ˜¯ pathlib.Pathï¼‰    
""")
parser.add_argument("-de", "--dump-exec", action="store_true", help="å¯¹ -d/--dump ä¼ å…¥çš„ä»£ç ç”¨ exec è¿è¡Œï¼Œå…¶ä¸­å¿…é¡»å­˜åœ¨åä¸º dump çš„å‡½æ•°ã€‚å¦åˆ™ï¼Œè§†ä¸ºè¡¨è¾¾å¼æˆ– lambda å‡½æ•°")
parser.add_argument("-s", "--select", default="", help="å¯¹è·¯å¾„è¿›è¡Œç­›é€‰ï¼Œæä¾›ä¸€ä¸ªè¡¨è¾¾å¼ï¼ˆä¼šæ³¨å…¥ä¸€ä¸ªå˜é‡ pathï¼Œç±»å‹æ˜¯ pathlib.Pathï¼‰æˆ–å‡½æ•°ï¼ˆä¼šä¼ å…¥ä¸€ä¸ªå‚æ•°ï¼Œç±»å‹æ˜¯ pathlib.Pathï¼‰")
parser.add_argument("-se", "--select-exec", action="store_true", help="å¯¹ -s/--select ä¼ å…¥çš„ä»£ç ç”¨ exec è¿è¡Œï¼Œå…¶ä¸­å¿…é¡»å­˜åœ¨åä¸º select çš„å‡½æ•°ã€‚å¦åˆ™ï¼Œè§†ä¸ºè¡¨è¾¾å¼æˆ– lambda å‡½æ•°")
parser.add_argument("-o", "--output-file", help="ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ­¤æ—¶å‘½ä»¤è¡Œä¼šè¾“å‡ºè¿›åº¦æ¡")
parser.add_argument("-hs", "--hashes", choices=(*algorithms_available, "crc32"), nargs="*", help="è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªç®—æ³•")
parser.add_argument("-dfs", "--depth-first", action="store_true", help="ä½¿ç”¨æ·±åº¦ä¼˜å…ˆæœç´¢ï¼Œå¦åˆ™ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆ")
parser.add_argument("-fl", "--follow-symlinks", action="store_true", help="è·Ÿè¿›ç¬¦å·è¿æ¥ï¼Œå¦åˆ™ä¼šæŠŠç¬¦å·é“¾æ¥è§†ä¸ºæ–‡ä»¶ï¼Œå³ä½¿å®ƒæŒ‡å‘ç›®å½•")
parser.add_argument("-v", "--version", action="store_true", help="è¾“å‡ºç‰ˆæœ¬å·")

from binascii import crc32
from collections.abc import Callable, Iterator, Sequence
from functools import partial
from hashlib import new as hashnew
from os import fsdecode, lstat, stat, stat_result, PathLike
from os.path import abspath, isdir, islink, relpath
from operator import attrgetter
from pathlib import Path
from sys import stdout
from textwrap import dedent
from typing import cast, Any, Optional


STAT_FIELDS = tuple(
    f for f in dir(stat_result) 
    if f.startswith("st_")
)


def stat_to_dict(
    path: None | bytes | str | PathLike = None, 
    /, 
    follow_symlinks: bool = False, 
) -> Optional[dict]:
    getstat: Callable[[bytes | str | PathLike], stat_result]
    if follow_symlinks:
        getstat = stat
    else:
        getstat = lstat
    try:
        return dict(zip(
            STAT_FIELDS, 
            attrgetter(*STAT_FIELDS)(getstat(path or ".")), 
        ))
    except OSError:
        return None


def file_multi_hashes(
    path: bytes | str | PathLike, 
    hashes: Sequence[str], 
) -> Optional[dict[str, str]]:
    try:
        file = open(path, "rb", buffering=0)
    except OSError:
        return None
    cache: dict[str, Any] = {alg: 0 if alg == "crc32" else hashnew(alg) for alg in hashes}
    updates = tuple(
        (lambda data: 
            cache.__setitem__("crc32", crc32(data, cast(int, cache["crc32"])))
        ) if alg == "crc32" else val.update 
        for alg, val in cache.items()
    )
    readinto = file.readinto
    buf = bytearray(1 << 16) # 64 KB
    view = memoryview(buf)
    while (size := readinto(buf)):
        for update in updates:
            update(view[:size])
    return {alg: format(val, "x") if alg == "crc32" else val.hexdigest() for alg, val in cache.items()}


def main():
    args = parser.parse_args()

    if args.version:
        from iterdir import __version__
        print(".".join(map(str, __version__)))
        raise SystemExit(0)

    from iterdir import iterdir, DirEntry

    predicate: Optional[Callable[[DirEntry], Optional[bool]]] = None
    select_code = dedent(args.select).strip()
    if select_code:
        ns = {"re": __import__("re")}
        if args.select_exec:
            exec(select_code, ns)
            select = ns.get("select")
        elif select_code.startswith("lambda "):
            select = eval(select_code, ns)
        else:
            select = eval("lambda path:" + select_code, ns)
        if callable(select):
            predicate = lambda e: select(Path(fsdecode(e)))

    dumps: Optional[Callable[[DirEntry], Any]] = None
    dump_code = dedent(args.dump).strip()
    if dump_code:
        ns = {}
        if args.dump_exec:
            exec(dump_code, ns)
            dump = ns.get("dump")
        elif dump_code.startswith("lambda "):
            dump = eval(dump_code, ns)
        else:
            dump = eval("lambda path:" + dump_code, ns)
        if callable(dump):
            dumps = lambda e: dump(Path(fsdecode(e)))

    path = args.path
    path_it: Iterator[DirEntry] = iterdir(
        DirEntry(path), 
        topdown=True if args.depth_first else None, 
        min_depth=args.min_depth, 
        max_depth=args.max_depth, 
        predicate=predicate, 
        follow_symlinks=args.follow_symlinks, 
    )

    output_file = args.output_file
    if output_file:
        from collections import deque
        from time import perf_counter

        def format_time(t):
            m, s = divmod(t, 60)
            if m < 60:
                return f"{m:02.0f}:{s:09.06f}"
            h, m = divmod(m, 60)
            if h < 24:
                return f"{h:02.0f}:{m:02.0f}:{s:09.06f}"
            d, h = divmod(h, 60)
            return f"{d}d{h:02.0f}:{m:02.0f}:{s:09.06f}"

        def progress(it):
            write = stdout.buffer.raw.write # type: ignore
            dq: deque[tuple[int, float]] = deque(maxlen=10*60)
            push = dq.append
            total = 0
            ndirs = 0
            nfiles = 0
            start_t = last_t = perf_counter()
            write(f"\r\x1b[KğŸ—‚ï¸  {total} = ğŸ“‚ {ndirs} + ğŸ“ {nfiles}".encode())
            push((total, start_t))
            for p in it:
                total += 1
                if p.is_dir():
                    ndirs += 1
                else:
                    nfiles += 1
                cur_t = perf_counter()
                if cur_t - last_t > 0.1:
                    speed = (total - dq[0][0]) / (cur_t - dq[0][1])
                    write(f"\r\x1b[KğŸ—‚ï¸  {total} = ğŸ“‚ {ndirs} + ğŸ“ {nfiles} | ğŸ•™ {format_time(cur_t-start_t)} | ğŸš€ {speed:.3f} it/s".encode())
                    push((total, cur_t))
                    last_t = cur_t
                yield p
            cur_t = perf_counter()
            speed = total / (cur_t - start_t)
            write(f"\r\x1b[KğŸ—‚ï¸  {total} = ğŸ“‚ {ndirs} + ğŸ“ {nfiles} | ğŸ•™ {format_time(cur_t-start_t)} | ğŸš€ {speed:.3f} it/s".encode())
        file = open(output_file, "w")
        path_it = iter(progress(path_it))
        write = file.buffer.write
    else:
        file = stdout # type: ignore
        write = file.buffer.raw.write # type: ignore

    if dumps is not None:
        try:
            for path in path_it:
                result = dumps(path)
                if not (result is None or isinstance(result, bytes)):
                    result = bytes(str(result), "utf-8")
                if result:
                    write(result)
                    write(b"\n")
        except KeyboardInterrupt:
            pass
        except BrokenPipeError:
            from sys import stderr
            stderr.close()
        finally:
            file.close()
    else:
        top = abspath(path)
        fmap: dict[str, Callable] = {
            "inode": DirEntry.inode, 
            "name": lambda e: e.name, 
            "path": lambda e: e.path, 
            "relpath": lambda e: relpath(abspath(e), top), 
            "isdir": isdir, 
            "islink": islink, 
            "stat": stat_to_dict, 
        }

        keys: list[str] = args.keys
        if keys:
            fmap = {k: fmap[k] for k in keys if k in fmap}
            keys = list(fmap)
        else:
            keys = KEYS

        if args.hashes:
            keys.append("hashes")
            fmap["hashes"] = partial(file_multi_hashes, hashes=args.hashes)

        records = ({k: f(e) for k, f in fmap.items()} for e in path_it)

        output_type = args.output_type
        json_dumps: Callable[..., bytes]
        if output_type in ("log", "json"):
            try:
                from orjson import dumps as json_dumps
            except ImportError:
                odumps: Callable
                try:
                    from ujson import dumps as odumps
                except ImportError:
                    from json import dumps as odumps
                json_dumps = lambda obj, /: bytes(odumps(obj, ensure_ascii=False), "utf-8")

        try:
            if output_type == "json":
                write(b"[")
                for i, record in enumerate(records):
                    if i:
                        write(b", ")
                    write(json_dumps(record))
                write(b"]")
            elif output_type == "log":
                for record in records:
                    write(json_dumps(record))
                    write(b"\n")
            else:
                from csv import DictWriter

                writer = DictWriter(file, fieldnames=keys)
                writer.writeheader()
                for record in records:
                    writer.writerow(record)
        except KeyboardInterrupt:
            pass
        except BrokenPipeError:
            from sys import stderr
            stderr.close()
        finally:
            file.close()


if __name__ == "__main__":
    from pathlib import Path
    from sys import path

    path[0] = str(Path(__file__).parents[1])
    main()

